<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pratik Kunapuli </title> <meta name="author" content="Pratik Kunapuli"> <meta name="description" content="&lt;strong&gt;Ph.D. Student&lt;/strong&gt;, University of Pennsylvania"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pratikkunapuli.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%70%72%61%74%69%6B%6B@%73%65%61%73.%75%70%65%6E%6E.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-0601-9787#%20your%20ORCID%20ID" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=GPJ2qqgAAAAJ&amp;hl=en&amp;oi=ao" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/PratikKunapuli" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/pratik-kunapuli-87485bb6" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/PKunapuli" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Projects </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/EQTrackingControl/">Equivariance in Tracking Control (ICRA 2025)</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Pratik Kunapuli </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic.jpg" sizes="(min-width: 1000px) 291.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.jpg?82605b92b2b3aa370ddbdf00682dcf28" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>I’m a doctoral student in Computer and Information Science (CIS) at <a href="https://www.grasp.upenn.edu/" rel="external nofollow noopener" target="_blank">GRASP</a> at the University of Pennsylvania. I am co-advised by <a href="https://www.kumarrobotics.org/" rel="external nofollow noopener" target="_blank">Vijay Kumar</a> and <a href="https://www.seas.upenn.edu/~dineshj/" rel="external nofollow noopener" target="_blank">Dinesh Jayaraman</a>, and am grateful to be supported by the NSF as a <a href="https://www.nsfgrfp.org/" rel="external nofollow noopener" target="_blank">GRFP</a> Fellow. My research interests lie at the intersection of machine learning and robotics, specifically related to using reinforcement learning for control of agile and dynamic systems as well as transferring learning approaches from simulation to the real world (sim2real).</p> <p>Previously, I completed both my BSc and MSc at Georgia Institute of Technology in Electrical and Computer Engineering. During my time there, I worked with <a href="https://www.me.gatech.edu/faculty/young" rel="external nofollow noopener" target="_blank">Aaron Young</a> in the <a href="http://www.epic.gatech.edu/" rel="external nofollow noopener" target="_blank">Exoskeleton Prosthetic Intelligent Controls Lab (EPIC Lab)</a> initially developing controllers for a powered knee and ankle prosthetic and eventually applying machine learning-based state estimation to autonomous hip exoskeletons. My work culminated in my Masters thesis, “Online Adaptation of User State Estimation in a Powered Hip Exoskeleton using Machine Learning”.</p> <p><a href="/assets/pdf/Pratik_Kunapuli_CV.pdf">Curriculum Vitae</a></p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Feb 10, 2025</th> <td> Two papers were accepted to ICRA 2025 in Atlanta this year! <a href="https://pratikkunapuli.github.io/EQTrackingControl/" rel="external nofollow noopener" target="_blank">Leveraging Symmetry to Accelerate Learning of Trajectory Tracking Controllers for Free-Flying Robotic Systems</a> and <a href="https://www.anishbhattacharya.com/research/vitfly" rel="external nofollow noopener" target="_blank">Vision Transformers for End-to-End Vision-Based Quadrotor Obstacle Avoidance”</a> will be presented as posters in person. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 14, 2024</th> <td> Honered that our work <a href="https://pratikkunapuli.github.io/EQTrackingControl/" rel="external nofollow noopener" target="_blank">Leveraging Symmetry to Accelerate Learning of Trajectory Tracking Controllers for Free-Flying Robotic Systems</a> was awarded the Best Paper Award in the Neuroscience and Interpretability track in the NeurReps Workshop at NeurIPS 2024! This is one of three best paper awards from the workshop, and this work wouldn’t have been possible without the oustanding work of my co-first-authors <a href="https://jakewelde.com" rel="external nofollow noopener" target="_blank">Jake Welde</a> and Nishanth Rao! </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 29, 2024</th> <td> <a href="https://pratikkunapuli.github.io/EQTrackingControl/" rel="external nofollow noopener" target="_blank">Leveraging Symmetry to Accelerate Learning of Trajectory Tracking Controllers for Free-Flying Robotic Systems</a> was accepted as an oral presentation at the NeurReps workshop at NeurIPS 2024! </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 11, 2024</th> <td> <a href="https://pratikkunapuli.github.io/EQTrackingControl/" rel="external nofollow noopener" target="_blank">Leveraging Symmetry to Accelerate Learning of Trajectory Tracking Controllers for Free-Flying Robotic Systems</a> will be presented at the SymRob workshop at IROS 2024! </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> </div> <div id="welde2024leveragingsymmetryacceleratelearning" class="col-sm-8"> <div class="title">Leveraging Symmetry to Accelerate Learning of Trajectory Tracking Controllers for Free-Flying Robotic Systems</div> <div class="author"> <em>Pratik Kunapuli<sup>*</sup></em>, Jake Welde<sup>*</sup>, Nishanth Rao<sup>*</sup>, Dinesh Jayaraman, and Vijay Kumar </div> <div class="periodical"> <em>ICRA</em>, IROS 2024 (Equivariant Robotics Workshop); NeurIPS 2024 (NeurReps Workshop) - Oral (Best Paper Award), 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2409.11238" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2409.11238" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://pratikkunapuli.github.io/EQTrackingControl/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Oral, Best Paper Award at NeurReps 2024</p> </div> <div class="abstract hidden"> <p>Tracking controllers enable robotic systems to accurately follow planned reference trajectories. In particular, reinforcement learning (RL) has shown promise in the synthesis of controllers for systems with complex dynamics and modest online compute budgets. However, the poor sample efficiency of RL and the challenges of reward design make training slow and sometimes unstable, especially for high-dimensional systems. In this work, we leverage the inherent Lie group symmetries of robotic systems with a floating base to mitigate these challenges when learning tracking controllers. We model a general tracking problem as a Markov decision process (MDP) that captures the evolution of both the physical and reference states. Next, we prove that symmetry in the underlying dynamics and running costs leads to an MDP homomorphism, a mapping that allows a policy trained on a lower-dimensional "quotient" MDP to be lifted to an optimal tracking controller for the original system. We compare this symmetry-informed approach to an unstructured baseline, using Proximal Policy Optimization (PPO) to learn tracking controllers for three systems: the Particle (a forced point mass), the Astrobee (a fullyactuated space robot), and the Quadrotor (an underactuated system). Results show that a symmetry-aware approach both accelerates training and reduces tracking error after the same number of training steps.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICRA</abbr> </div> <div id="bhattacharya2024visiontransformersendtoendvisionbased" class="col-sm-8"> <div class="title">Vision Transformers for End-to-End Vision-Based Quadrotor Obstacle Avoidance</div> <div class="author"> Anish Bhattacharya, Nishanth Rao, Dhruv Parikh, <em>Pratik Kunapuli</em>, Nikolai Matni, and Vijay Kumar </div> <div class="periodical"> <em>ICRA</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2405.10391" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2405.10391" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We demonstrate the capabilities of an attention-based end-to-end approach for high-speed quadrotor obstacle avoidance in dense, cluttered environments, with comparison to various state-of-the-art architectures. Quadrotor unmanned aerial vehicles (UAVs) have tremendous maneuverability when flown fast; however, as flight speed increases, traditional vision-based navigation via independent mapping, planning, and control modules breaks down due to increased sensor noise, compounding errors, and increased processing latency. Thus, learning-based, end-to-end planning and control networks have shown to be effective for online control of these fast robots through cluttered environments. We train and compare convolutional, U-Net, and recurrent architectures against vision transformer models for depth-based end-to-end control, in a photorealistic, high-physics-fidelity simulator as well as in hardware, and observe that the attention-based models are more effective as quadrotor speeds increase, while recurrent models with many layers provide smoother commands at lower speeds. To the best of our knowledge, this is the first work to utilize vision transformers for end-to-end vision-based quadrotor control.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE RAL</abbr> </div> <div id="kang2021realtime" class="col-sm-8"> <div class="title">Real-Time Gait Phase Estimation for Robotic Hip Exoskeleton Control During Multimodal Locomotion</div> <div class="author"> Inseung Kang, Dean D. Molinaro, Srijan Duggal, Yanrong Chen, <em>Pratik Kunapuli</em>, and Aaron J. Young </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9364364" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/kang2021realtime.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We developed and validated a gait phase estimator for real-time control of a robotic hip exoskeleton during multimodal locomotion. Gait phase describes the fraction of time passed since the previous gait event, such as heel strike, and is a promising framework for appropriately applying exoskeleton assistance during cyclic tasks. A conventional method utilizes a mechanical sensor to detect a gait event and uses the time since the last gait event to linearly interpolate the current gait phase. While this approach may work well for constant treadmill walking, it shows poor performance when translated to overground situations where the user may change walking speed and locomotion modes dynamically. To tackle these challenges, we utilized a convolutional neural network-based gait phase estimator that can adapt to different locomotion mode settings to modulate the exoskeleton assistance. Our resulting model accurately predicted the gait phase during multimodal locomotion without any additional information about the user’s locomotion mode, with a gait phase estimation RMSE of 5.04 ± 0.79%, significantly outperforming the literature standard (p &lt;; 0.05). Our study highlights the promise of translating exoskeleton technology to more realistic settings where the user can naturally and seamlessly navigate through different terrain settings.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">IEEE TMRB</abbr> </div> <div id="kang2020realtime" class="col-sm-8"> <div class="title">Real-Time Neural Network-Based Gait Phase Estimation Using a Robotic Hip Exoskeleton</div> <div class="author"> <em>Pratik Kunapuli<sup>*</sup></em>, Inseung Kang<sup>*</sup>, and Aaron J. Young </div> <div class="periodical"> <em>IEEE Transactions on Medical Robotics and Bionics</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/8941004" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/kang2020realtime.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Lower limb exoskeletons provide assistance during the gait cycle using a state variable, one in particular is gait phase. This is crucial for the exoskeleton controller to provide the user accurate assistance. Conventional methods often utilize an event marker to estimate gait phase by computing the average stride time. However, this strategy has limitations in adapting to dynamic speeds. We developed a sensor fusion-based neural network model to estimate the gait phase in real-time that can adapt to dynamic speeds ranging from 0.6 to 1.1 m/s. Ten able-bodied subjects walked with an exoskeleton using our estimator and were provided with corresponding torque assistance. Our best performing model had RMSE below 29 ms and 4% for real-time estimation and torque generation, respectively, reducing the estimation error by 36.0% (p &lt; 0.01) and torque error by 40.9% (p &lt; 0.001) compared to conventional methods. Our results indicate that creating a general user-independent model and additionally training on user-specific data outperforms the user-specific model and user-independent model. Our study validates the feasibility of using a sensor fusion-based machine learning model to accurately estimate the user’s gait phase and improve the controllability of a lower limb exoskeleton.</p> </div> </div> </div> </li> </ol> </div> <h2> <a href="#mentees" style="color: inherit">Mentored Students</a> </h2> <div class="mentees"> <h4>Current Students</h4> <div class="mentee"> <span>Nishanth Rao</span> <span class="separator">:</span> Robo MS, UPenn </div> <div class="mentee"> <span>Bryan Alfaro</span> <span class="separator">:</span> Robo MS, UPenn </div> <h4>Past Students</h4> <div class="mentee"> <span>Harsh Goel</span> <span class="separator">:</span> Robo MS, UPenn <span class="separator">→</span> PhD, UT Austin </div> <div class="mentee"> <span>Akshay Manikandan</span> <span class="separator">:</span> Robo MS, UPenn <span class="separator">→</span> Tech Consultant, Birlasoft </div> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%70%72%61%74%69%6B%6B@%73%65%61%73.%75%70%65%6E%6E.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-0601-9787#%20your%20ORCID%20ID" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=GPJ2qqgAAAAJ&amp;hl=en&amp;oi=ao" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/PratikKunapuli" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/pratik-kunapuli-87485bb6" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/PKunapuli" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Pratik Kunapuli. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: February 10, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>